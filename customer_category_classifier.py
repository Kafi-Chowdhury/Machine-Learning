# -*- coding: utf-8 -*-
"""project cse422 part 2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11P3nRSo_ozcYbs8VkKVzDabQ0qaDe99N
"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import LabelEncoder as le
from sklearn.preprocessing import StandardScaler as ss
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score, classification_report

df=pd.read_csv('/content/drive/MyDrive/Customer_Category_Classifier_Dataset.csv')
df

import seaborn as sns
import matplotlib.pyplot as plt

corr=df.corr(numeric_only=True)
plt.figure(figsize=(30,20))
sns.heatmap(corr,annot=True,cmap="coolwarm",center=0)
plt.title("Correlation Heatmap of Features")
plt.show()

df.head()

df.tail()

df=df.drop(columns=['ID'])

#target= segmentation (A,B,C,D)
#classification problem , multiclass classification because (A,B,C,D)
df.size

row , col = df.shape
print('row:', row)
print('col & features (including target):', col)
print('features (excluding target):', col-1)

df.dtypes
#both quantitative & categorical

df['Segmentation'].value_counts().sort_index()
#imbalance

labels=['A','B','C','D']
values=[1972,1858,1970,2268]

plt.bar(labels,values)
plt.xlabel("labels")
plt.ylabel("values")
plt.show()

"""#DATA PREPROCESSING"""

df.isnull().sum() #total count of null values column-wise

#Categorical Values
print(df['Ever_Married'].dtype)
print(df['Graduated'].dtype)
print(df['Profession'].dtype)
print(df['Var_1'].dtype)

#Numeric Values
print(df['Work_Experience'].dtype)
print(df['Family_Size'].dtype)

print('row wise data missing:',df.isnull().any(axis=1).sum()) #data missing row wise
print('column wise data missing:',df.isnull().any(axis=0).sum()) #data missing column wise
print('total length of the dataset:',len(df)) #total length of dataset

#median for numerical values
df['Work_Experience']=df['Work_Experience'].fillna(df['Work_Experience'].median())
df['Family_Size']=df['Family_Size'].fillna(df['Family_Size'].median())

#mode for categorical values
df['Ever_Married']=df['Ever_Married'].fillna(df['Ever_Married'].mode()[0])
df['Graduated']=df['Graduated'].fillna(df['Graduated'].mode()[0])
df['Profession']=df['Profession'].fillna(df['Profession'].mode()[0])
df['Var_1']=df['Var_1'].fillna(df['Var_1'].mode()[0])

# null values are gone
df.isnull().sum()

df.head(30)

"""#label encoding"""

#Categorical Values
print(df['Ever_Married'].dtype)
print(df['Graduated'].dtype)
print(df['Profession'].dtype)
print(df['Var_1'].dtype)

encoder = le()
df['Gender']=encoder.fit_transform(df['Gender'])
df['Ever_Married']=encoder.fit_transform(df['Ever_Married'])
df['Graduated']=encoder.fit_transform(df['Graduated'])
df['Segmentation']=encoder.fit_transform(df['Segmentation'])

df.head()

df.dtypes

df.head()

"""#One-Hot encoding"""

profession_enc = pd.get_dummies(df['Profession'], dtype=int)
Var_1_enc = pd.get_dummies(df['Var_1'], dtype=int)
Spending_Score_enc = pd.get_dummies(df['Spending_Score'], dtype=int)
df = pd.concat([df.drop(['Profession','Var_1','Spending_Score'], axis=1),profession_enc, Var_1_enc, Spending_Score_enc], axis=1)

df.head()

df.dtypes

"""#Feature Scaling"""

print(round(df.var(),2))

scaler=ss()
df[['Age','Work_Experience','Family_Size']]=scaler.fit_transform(df[['Age','Work_Experience','Family_Size']])
print(round(df.var(),2))

"""#Data Splitting"""

df['Segmentation'].value_counts().sort_index()

X=df.drop('Segmentation',axis=1)
y=df['Segmentation']
# Stratified split
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42,stratify=y)
print(X_train.shape)
print(X_test.shape)

models={"KNN": KNeighborsClassifier(n_neighbors=5),"Logistic Regression": LogisticRegression(max_iter=1000, random_state=42),"Neural Network": MLPClassifier(hidden_layer_sizes=(64,32), max_iter=500, random_state=42)}
results={}
for name,model in models.items():
  model.fit(X_train,y_train)
  y_pred=model.predict(X_test)
  acc=accuracy_score(y_test,y_pred)
  results[name]=acc
  print(f"\n{name} Accuracy: {acc:.4f}")
  print(classification_report(y_test, y_pred))

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import (accuracy_score,precision_score,recall_score,f1_score,classification_report,confusion_matrix,roc_auc_score,roc_curve)
from sklearn.preprocessing import label_binarize
metrics={"Model":[],"Accuracy":[],"Precision":[],"Recall":[],"F1":[],"AUC":[]}

# Classes
classes=sorted(y.unique())

for name,model in models.items():
  model.fit(X_train,y_train)
  y_pred=model.predict(X_test)
  # Metrics
  acc=accuracy_score(y_test,y_pred)
  prec=precision_score(y_test,y_pred,average="weighted",zero_division=0)
  rec=recall_score(y_test,y_pred,average="weighted",zero_division=0)
  f1=f1_score(y_test,y_pred,average="weighted",zero_division=0)
  # ROC-AUC (one-vs-rest for multiclass)
  y_test_bin=label_binarize(y_test, classes=classes)
  y_pred_prob=model.predict_proba(X_test) if hasattr(model,"predict_proba") else None
  auc=roc_auc_score(y_test_bin,y_pred_prob,average="macro", multi_class="ovr") if y_pred_prob is not None else None

  metrics["Model"].append(name)
  metrics["Accuracy"].append(acc)
  metrics["Precision"].append(prec)
  metrics["Recall"].append(rec)
  metrics["F1"].append(f1)
  metrics["AUC"].append(auc)
  # Confusion Matrix
  cm=confusion_matrix(y_test,y_pred,labels=classes)
  plt.figure(figsize=(5,4))
  sns.heatmap(cm,annot=True,fmt="d",cmap="Blues",xticklabels=classes,yticklabels=classes)
  plt.title(f"Confusion Matrix - {name}")
  plt.xlabel("Predicted")
  plt.ylabel("Actual")
  plt.show()
  #ROC Curve (only if predict_proba available)
  if y_pred_prob is not None:
    plt.figure(figsize=(6,4))
    for i, cls in enumerate(classes):
      fpr, tpr, _ = roc_curve(y_test_bin[:, i],y_pred_prob[:, i])
      plt.plot(fpr, tpr,label=f"Class {cls}")
    plt.plot([0,1],[0,1],'k--')
    plt.title(f"ROC Curve - {name}")
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.legend()
    plt.show()

# Final comparison DataFrame
results_df=pd.DataFrame(metrics)
print(results_df)
# Accuracy bar chart
plt.figure(figsize=(6,4))
sns.barplot(data=results_df,x="Model",y="Accuracy")
plt.title("Model Accuracy Comparison")
plt.show()

from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

# Only use input features (exclude target Segmentation)
X_unsupervised = df.drop("Segmentation", axis=1)

# Apply KMeans with 4 clusters (since target has 4 classes A,B,C,D)
kmeans = KMeans(n_clusters=4, random_state=42)
df['Cluster'] = kmeans.fit_predict(X_unsupervised)

# Count customers in each cluster
print(df['Cluster'].value_counts())

# Compare clusters vs actual Segmentation
pd.crosstab(df['Cluster'], df['Segmentation'])

from sklearn.decomposition import PCA

# Reduce features to 2D for visualization
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_unsupervised)

plt.figure(figsize=(8,6))
plt.scatter(X_pca[:,0], X_pca[:,1], c=df['Cluster'], cmap='viridis', alpha=0.6)
plt.title("KMeans Clustering of Customers (2D PCA Projection)")
plt.xlabel("Principal Component 1")
plt.ylabel("Principal Component 2")
plt.colorbar(label="Cluster")
plt.show()

from sklearn.metrics import silhouette_score

score = silhouette_score(X_unsupervised, df['Cluster'])
print("Silhouette Score:", score)